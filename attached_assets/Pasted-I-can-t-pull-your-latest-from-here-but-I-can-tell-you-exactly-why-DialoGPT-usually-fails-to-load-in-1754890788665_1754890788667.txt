I can’t pull your latest from here, but I can tell you exactly why DialoGPT usually fails to load in this setup and how to confirm/fix it fast. The failure almost always comes from one of these (ranked by likelihood given your recent changes):


---

1) The repo ID is wrong (or wrong vendor)

Symptom: instant 404s on model files, or transformers.js tries to fetch from HF and never starts.

Correct ID for transformers.js is typically the Xenova-converted model, e.g.
Xenova/DialoGPT-medium (or Xenova/DialoGPT-small)
Not microsoft/DialoGPT-* and not xenova/dialogpt* (case matters).

If your manifest or “get random model” uses a lowercased dialogpt, the loader will fail.


Fix: in your server manifest / allowlist, add the exact casing:

{ repo_id: 'Xenova/DialoGPT-medium', task: 'text-generation', name: 'DialoGPT (Medium)' }

And in the client, be sure you pass that exact repo_id.


---

2) Your chat-only allowlist is blocking DialoGPT

If you recently enforced “chat only,” you probably added a server allowlist. If Xenova/DialoGPT-medium isn’t in it:

Symptom: POST /api/models/Xenova/DialoGPT-medium/download → 403 (or your code swallows it), and no subsequent /models/... file requests.

Quick check (DevTools → Network):

Click your dialoGPT load action. Do you see 403 on the download route? If yes, it’s the allowlist.


Fix: add to allowlist and rebuild:

const CURATED_MODELS = [
  { repo_id: 'Xenova/tinyllama-1.1b-chat-v1.0', task: 'text-generation', name: 'TinyLlama 1.1B Chat' },
  { repo_id: 'Xenova/DialoGPT-medium', task: 'text-generation', name: 'DialoGPT (Medium)' },
  // optionally: { repo_id: 'Xenova/DialoGPT-small', task: 'text-generation', name: 'DialoGPT (Small)' },
];
const ALLOW = new Set(CURATED_MODELS.map(m => m.repo_id));


---

3) Pipeline task mismatch

DialoGPT is a text-generation model. If you switched your loader to 'conversational' or 'chat-completion', it won’t load.

Fix:

const pipe = await pipeline('text-generation', 'Xenova/DialoGPT-medium', { quantized: true });


---

4) Local model path / file serving is off

If you pinned transformers.js to local files but your /models route doesn’t actually contain DialoGPT yet:

Symptom: a bunch of 404s under /models/... after your status says “Ready”.

Fix:

Ensure you kick off a download before loading:

POST /api/models/Xenova/DialoGPT-medium/download

poll /api/models/.../status until isAvailable: true


Make sure your server is serving the folder you download to (ObjectStorageService search paths).

In the client, point transformers.js to your route:

import { env } from '@xenova/transformers';
env.localModelPath = '/models';
// env.allowRemoteModels = false; // optional strict mode



---

5) Device memory / size too big (mobile)

DialoGPT-medium is much larger than DistilGPT-2. On a phone, WebGPU/WASM may run out of memory or the adapter gets lost.

Symptoms: long stall and then a generic “out of memory,” “adapter lost,” or a silent fail with no tokens.

Fixes/workarounds:

Try Xenova/DialoGPT-small first (or stay on TinyLlama-chat if it fits).

Lower thread count for WASM (if you’re falling back), or ensure WebGPU is actually used.

Keep only one model loaded at a time; release previous instances.



---

90-second checklist (what to do now)

1. Network tab while loading DialoGPT:

Do you see GET /api/manifest → POST /api/models/.../download → repeated GET /api/models/.../status → a burst of GET /models/... ?

If 403 on /download: it’s the allowlist (fix #2).

If 404 on /models/...: it’s localModelPath/serving (fix #4).

If nothing after manifest: wrong repo ID or client-side filter returns empty (fix #1/#3).



2. Console logs: add these logs to pinpoint:

console.log('[ModelPicker] selected', model.repo_id, model.task);
console.log('[Loader] pipeline task:', model.task, 'repo:', model.repo_id);


3. Try small first: Xenova/DialoGPT-small on your phone. If that loads and medium doesn’t, it’s memory (fix #5).




---

Minimal code you can paste

Server manifest/allowlist (add DialoGPT):


const CURATED_MODELS = [
  { repo_id: 'Xenova/tinyllama-1.1b-chat-v1.0', task: 'text-generation', name: 'TinyLlama 1.1B Chat' },
  { repo_id: 'Xenova/DialoGPT-small', task: 'text-generation', name: 'DialoGPT (Small)' },
  { repo_id: 'Xenova/DialoGPT-medium', task: 'text-generation', name: 'DialoGPT (Medium)' },
];
const ALLOW = new Set(CURATED_MODELS.map(m => m.repo_id));

Client loader (ensure correct task + local path):


import { pipeline, env } from '@xenova/transformers';
env.localModelPath = '/models'; // served by your server

const pipe = await pipeline('text-generation', selected.repo_id, { quantized: true });

If you paste your manifest JSON, your download/status responses, or the exact Network errors when DialoGPT fails, I’ll point to the exact line to change. My bet, given your last “chat-only” push, is either (a) allowlist missing Xenova/DialoGPT-medium, or (b) a tiny casing mismatch in the repo ID.

